import os
import platform
import sys

# protobuf í˜¸í™˜ì„±ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“  importë³´ë‹¤ ë¨¼ì €!)
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

# sqlite3 íŒ¨ì¹˜ (ë°°í¬ í™˜ê²½ ê³ ë ¤)
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    print("<<<<< sqlite3 patched with pysqlite3 >>>>>")
except ImportError:
    # pysqlite3ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ sqlite3 ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ì •ìƒ)
    print("<<<<< using default sqlite3 >>>>>")

print(f"<<<<< app.py IS BEING LOADED on {platform.system()} >>>>>")

import streamlit as st
import tempfile
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from typing import Dict, List, Any
import time
import uuid
import subprocess
import shutil
import glob
import gc
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

# from streamlit_extras.buy_me_a_coffee import button

# ìˆ˜ë™ìœ¼ë¡œ Buy Me a Coffee ë²„íŠ¼ êµ¬í˜„
def add_buy_me_coffee_button():
    st.markdown(
        """
        <style>
        .coffee-button {
            text-align: left;
            margin-bottom: 20px;
        }
        .coffee-button img {
            height: 40px !important;
            width: 144px !important;
            max-height: 40px !important;
            max-width: 144px !important;
            object-fit: contain !important;
            border: none !important;
        }
        </style>
        <div class="coffee-button">
            <a href="https://www.buymeacoffee.com/ehtjd33e" target="_blank">
                <img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" 
                     alt="Buy Me A Coffee">
            </a>
        </div>
        """,
        unsafe_allow_html=True
    )

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="ë²•ë¥ ê°€ ì±—ë´‡", page_icon=":books:", layout="wide")

# ë²„íŠ¼ ì¶”ê°€ (ì œëª© ë°”ë¡œ ìœ„)
add_buy_me_coffee_button()

st.title("ğŸ“š ë²•ë¥ ê°€ ì±—ë´‡")
st.caption("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€ë°›ìœ¼ì„¸ìš”")

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")

# OS ê°ì§€ ë° í”Œë«í¼ë³„ ì„¤ì •
SYSTEM_OS = platform.system().lower()
IS_WINDOWS = SYSTEM_OS == 'windows'
IS_LINUX = SYSTEM_OS == 'linux'
IS_MACOS = SYSTEM_OS == 'darwin'

# ë°°í¬ í™˜ê²½ ê°ì§€ (Streamlit Cloud, Heroku ë“±)
IS_DEPLOYMENT = any([
    os.getenv('STREAMLIT_CLOUD'),
    os.getenv('HEROKU'),
    os.getenv('RAILWAY'),
    os.getenv('RENDER'),
    '/app' in os.getcwd(),  # ì¼ë°˜ì ì¸ ì»¨í…Œì´ë„ˆ ê²½ë¡œ
    '/home/appuser' in os.getcwd()  # Streamlit Cloud ê²½ë¡œ
])

print(f"ì‹œìŠ¤í…œ ì •ë³´: OS={SYSTEM_OS}, ë°°í¬í™˜ê²½={IS_DEPLOYMENT}")

# ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–‹")  # ì»¤ì„œ íš¨ê³¼
        time.sleep(0.01)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.container.markdown(self.text)  # ìµœì¢… í…ìŠ¤íŠ¸ (ì»¤ì„œ ì œê±°)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ë²•ë¥  ë¬¸ì„œë‚˜ ê´€ë ¨ ìë£Œë¥¼ PDF í˜•íƒœë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    st.divider()
    
    st.header("âš™ï¸ ì„¤ì •")
    if not openai_api_key:
        st.error("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        st.info("ğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:\nOPENAI_API_KEY=your_api_key_here")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìš”ì•½ ëª¨ë“œ ì„ íƒ ì¶”ê°€
    st.divider()
    st.subheader("ğŸš€ ìš”ì•½ ëª¨ë“œ")
    summary_mode = st.radio(
        "ìš”ì•½ ì†ë„ ì„ íƒ:",
        options=["ë¹ ë¥¸ ëª¨ë“œ âš¡", "ì •í™• ëª¨ë“œ ğŸ¯"],
        index=0,  # ê¸°ë³¸ê°’: ë¹ ë¥¸ ëª¨ë“œ
        help="ë¹ ë¥¸ ëª¨ë“œ: 2-3ë°° ë¹ ë¥¸ ì†ë„, ê°„ê²°í•œ ìš”ì•½\nì •í™• ëª¨ë“œ: ë” ìƒì„¸í•˜ê³  ì •í™•í•œ ìš”ì•½"
    )
    
    # ì„ íƒëœ ëª¨ë“œë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    if 'summary_fast_mode' not in st.session_state:
        st.session_state.summary_fast_mode = True
    
    st.session_state.summary_fast_mode = "ë¹ ë¥¸ ëª¨ë“œ" in summary_mode
    
    if uploaded_file:
        st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œë¨: {uploaded_file.name}")
        st.info(f"ğŸ“„ íŒŒì¼ í¬ê¸°: {uploaded_file.size / 1024:.1f} KB")

# í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ ChromaDB ì‚­ì œ í•¨ìˆ˜
def safe_delete_chromadb_cross_platform(target_dir, max_retries=3, delay=1):
    """ëª¨ë“  OSì—ì„œ ì•ˆì „í•˜ê²Œ ChromaDB í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    
    # ë¨¼ì € vectorstore ê°ì²´ í•´ì œ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    if 'vectorstore' in st.session_state:
        st.session_state.vectorstore = None
    gc.collect()
    
    if not os.path.exists(target_dir):
        return True
    
    # Path ê°ì²´ ì‚¬ìš©ìœ¼ë¡œ í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„± ê°œì„ 
    target_path = Path(target_dir)
    temp_name = f"{target_dir}_deleted_{uuid.uuid4().hex[:8]}"
    temp_path = Path(temp_name)
    
    try:
        # 1ë‹¨ê³„: í´ë” ì´ë¦„ ë³€ê²½ (ëª¨ë“  OSì—ì„œ ì•ˆì „)
        target_path.rename(temp_path)
        print(f"ChromaDB ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½: {target_dir} -> {temp_name}")
        
        # 2ë‹¨ê³„: í”Œë«í¼ë³„ ì‚­ì œ ì „ëµ
        success = False
        
        for attempt in range(max_retries):
            try:
                time.sleep(delay)
                
                if IS_WINDOWS and not IS_DEPLOYMENT:
                    # Windows ë¡œì»¬ í™˜ê²½: ê°•í™”ëœ ì‚­ì œ ë°©ë²•
                    success = _windows_force_delete(temp_path)
                else:
                    # Linux/macOS ë˜ëŠ” ë°°í¬ í™˜ê²½: í‘œì¤€ ë°©ë²•
                    success = _standard_delete(temp_path)
                
                if success:
                    print(f"ChromaDB ë””ë ‰í† ë¦¬ ì‚­ì œ ì„±ê³µ: {temp_name}")
                    return True
                    
            except Exception as e:
                print(f"ì‚­ì œ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {str(e)}")
                if attempt < max_retries - 1:
                    delay *= 2
        
        # ì‚­ì œ ì‹¤íŒ¨ ì‹œì—ë„ ì´ë¦„ ë³€ê²½ì€ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ True ë°˜í™˜
        print(f"ChromaDB ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨, ë‚˜ì¤‘ì— ì •ë¦¬ë¨: {temp_name}")
        return True
        
    except Exception as e:
        print(f"ChromaDB ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return False

def _windows_force_delete(path):
    """Windowsì—ì„œ ê°•í™”ëœ ì‚­ì œ ë°©ë²•"""
    try:
        # ë°©ë²• 1: í‘œì¤€ shutil ì‚­ì œ
        shutil.rmtree(path)
        return True
    except Exception:
        pass
    
    try:
        # ë°©ë²• 2: Windows ì „ìš© robocopy ì‚¬ìš© (robocopyê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if shutil.which("robocopy"):
            empty_dir = Path("./temp_empty_dir")
            empty_dir.mkdir(exist_ok=True)
            
            # robocopyë¡œ ë¹ˆ í´ë”ë¥¼ ë¯¸ëŸ¬ë§í•˜ì—¬ ì‚­ì œ íš¨ê³¼
            result = subprocess.run([
                "robocopy", str(empty_dir), str(path), "/MIR", "/NFL", "/NDL", "/NJH", "/NJS"
            ], capture_output=True, text=True)
            
            # ë¹ˆ í´ë”ë“¤ ì •ë¦¬
            empty_dir.rmdir()
            path.rmdir()
            return True
    except Exception:
        pass
    
    try:
        # ë°©ë²• 3: íŒŒì¼ë³„ ê°œë³„ ì‚­ì œ
        for root, dirs, files in os.walk(path, topdown=False):
            for file in files:
                os.chmod(os.path.join(root, file), 0o777)
                os.remove(os.path.join(root, file))
            for dir in dirs:
                os.rmdir(os.path.join(root, dir))
        os.rmdir(path)
        return True
    except Exception:
        pass
    
    return False

def _standard_delete(path):
    """í‘œì¤€ ì‚­ì œ ë°©ë²• (Linux/macOS/ë°°í¬í™˜ê²½)"""
    try:
        shutil.rmtree(path)
        return True
    except Exception as e:
        print(f"í‘œì¤€ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return False

# ì „ì²´ ChromaDB ì •ë¦¬ í•¨ìˆ˜ (í¬ë¡œìŠ¤ í”Œë«í¼)
def cleanup_all_chromadb():
    """ëª¨ë“  ChromaDB ê´€ë ¨ í´ë”ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ChromaDB ê´€ë ¨ í´ë” ì°¾ê¸°
        current_dir = Path.cwd()
        chroma_patterns = ["chroma_db*", ".chromadb*"]
        
        cleaned_count = 0
        for pattern in chroma_patterns:
            for chroma_path in current_dir.glob(pattern):
                if chroma_path.is_dir():
                    try:
                        success = safe_delete_chromadb_cross_platform(str(chroma_path))
                        if success:
                            cleaned_count += 1
                            print(f"ì •ë¦¬ë¨: {chroma_path}")
                    except Exception as e:
                        print(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ({chroma_path}): {str(e)}")
        
        if cleaned_count > 0:
            print(f"ì´ {cleaned_count}ê°œì˜ ChromaDB í´ë”ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
    except Exception as e:
        print(f"ChromaDB ì „ì²´ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

# ì•ˆì „í•œ ChromaDB ì‚­ì œ í•¨ìˆ˜ (ê¸°ë³¸ í˜¸í™˜)
def safe_delete_chromadb(max_retries=3, delay=1):
    """ChromaDB í´ë”ë¥¼ ì•ˆì „í•˜ê²Œ ì‚­ì œí•©ë‹ˆë‹¤."""
    return safe_delete_chromadb_cross_platform("./chroma_db", max_retries, delay)

# ì•± ì‹œì‘ ì‹œ ChromaDB ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)
if 'app_initialized' not in st.session_state:
    print(f"ì•± ì´ˆê¸°í™” ì‹œì‘ - OS: {SYSTEM_OS}, ë°°í¬í™˜ê²½: {IS_DEPLOYMENT}")
    
    # ì „ì²´ ChromaDB í´ë” ì •ë¦¬
    cleanup_all_chromadb()
    
    st.session_state.app_initialized = True
    print("ì•± ì´ˆê¸°í™” ì™„ë£Œ")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í¬ë¡œìŠ¤ í”Œë«í¼ ê°œì„  ë²„ì „)
@st.cache_resource
def initialize_rag_system(file_path):
    """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  vectorstoreì™€ ë¶„í• ëœ ë¬¸ì„œë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘: {file_path}")
        
        # PDF ë¬¸ì„œ ë¡œë”©
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        print(f"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(documents)}ê°œ í˜ì´ì§€")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬")
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # ê³ ìœ í•œ ChromaDB ë””ë ‰í† ë¦¬ ì‚¬ìš© (ì¶©ëŒ ë°©ì§€)
        timestamp = int(time.time())
        random_id = uuid.uuid4().hex[:8]
        chroma_dir = f"./chroma_db_{timestamp}_{random_id}"
        
        print(f"ìƒˆ ChromaDB ë””ë ‰í† ë¦¬: {chroma_dir}")
        
        # ê¸°ì¡´ chroma_db í´ë”ê°€ ìˆë‹¤ë©´ ì •ë¦¬
        old_chroma_dir = "./chroma_db"
        if os.path.exists(old_chroma_dir):
            try:
                safe_delete_chromadb_cross_platform(old_chroma_dir)
                print(f"ê¸°ì¡´ ChromaDB ë””ë ‰í† ë¦¬ ì •ë¦¬ë¨: {old_chroma_dir}")
            except Exception as e:
                print(f"ê¸°ì¡´ ChromaDB ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")

        # ìƒˆë¡œìš´ ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=chroma_dir
        )
        
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        # vectorstoreì™€ ë¶„í• ëœ ë¬¸ì„œë“¤ì„ í•¨ê»˜ ë°˜í™˜
        return vectorstore, splits
        
    except Exception as e:
        error_msg = f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        st.error(error_msg)
        return None, None

# ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (í¬ë¡œìŠ¤ í”Œë«í¼)
def save_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # í¬ë¡œìŠ¤ í”Œë«í¼ ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix="uploaded_") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            temp_path = tmp_file.name
        
        print(f"ì„ì‹œ íŒŒì¼ ìƒì„±: {temp_path}")
        return temp_path
        
    except Exception as e:
        error_msg = f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        st.error(error_msg)
        return None

# RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
def get_rag_response_streaming(question, vectorstore, api_key, container):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)."""
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = StreamlitCallbackHandler(container)
        
        # LLM ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0,
            streaming=True,
            callbacks=[callback_handler]
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¼ ê²½ìš° ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        response = qa_chain.invoke({"query": question})
        return callback_handler.text  # ìŠ¤íŠ¸ë¦¬ë°ëœ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_msg)
        container.error(error_msg)
        return error_msg

# ë¬¸ì„œ ìš”ì•½ í•¨ìˆ˜ ì¶”ê°€ (ì†ë„ ìµœì í™” ë²„ì „)
def generate_document_summary_improved(documents, api_key, fast_mode=True):
    """ì „ì²´ ë¬¸ì„œì˜ í¬ê´„ì ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤ (ì²­í¬ ìˆ˜ì— ë”°ë¼ ì ì‘ì  ë°©ì‹ ì‚¬ìš©)."""
    try:
        # LLM ì„¤ì • (ë¹ ë¥¸ ëª¨ë“œ ì‹œ ë” ë¹ ë¥¸ ì„¤ì •)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.1 if not fast_mode else 0.2,  # ë¹ ë¥¸ ëª¨ë“œì—ì„œëŠ” ì•½ê°„ ë†’ê²Œ
            timeout=60,
            max_retries=1 if fast_mode else 2  # ë¹ ë¥¸ ëª¨ë“œì—ì„œëŠ” ì¬ì‹œë„ ì¤„ì´ê¸°
        )
        
        print(f"ë¬¸ì„œ ìš”ì•½ ì‹œì‘: {len(documents)}ê°œ ì²­í¬ ({'ë¹ ë¥¸ ëª¨ë“œ' if fast_mode else 'ì •í™• ëª¨ë“œ'})")
        
        # ë¹ ë¥¸ ëª¨ë“œ ì‹œ ë” ê³µê²©ì ì¸ ì²­í¬ ìˆ˜ ê¸°ì¤€
        if fast_mode:
            small_threshold = 8
            medium_threshold = 20
        else:
            small_threshold = 10
            medium_threshold = 30
        
        # ì²­í¬ ìˆ˜ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ ì‚¬ìš©
        if len(documents) <= small_threshold:
            # ì‘ì€ ë¬¸ì„œ: stuff ë°©ì‹ìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬
            print("ì‘ì€ ë¬¸ì„œ ê°ì§€ - Stuff ë°©ì‹ ì‚¬ìš©")
            return _generate_summary_stuff_method(documents, llm, fast_mode)
        elif len(documents) <= medium_threshold:
            # ì¤‘ê°„ í¬ê¸° ë¬¸ì„œ: ë³‘ë ¬ ê·¸ë£¹í™” ë°©ì‹
            print("ì¤‘ê°„ í¬ê¸° ë¬¸ì„œ ê°ì§€ - ë³‘ë ¬ ê·¸ë£¹í™” ë°©ì‹ ì‚¬ìš©") 
            return _generate_summary_parallel_grouped_method(documents, llm, fast_mode)
        else:
            # í° ë¬¸ì„œ: ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ í›„ ìš”ì•½
            print("í° ë¬¸ì„œ ê°ì§€ - ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ë°©ì‹ ì‚¬ìš©")
            return _generate_summary_smart_sampling_method(documents, llm, fast_mode)
            
    except Exception as e:
        error_msg = f"ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
        print("ê¸°ë³¸ ìš”ì•½ ë°©ì‹ìœ¼ë¡œ í´ë°±...")
        return _generate_simple_summary(documents, llm)

def _generate_summary_stuff_method(documents, llm, fast_mode=True):
    """ì‘ì€ ë¬¸ì„œë¥¼ ìœ„í•œ Stuff ë°©ì‹ ìš”ì•½ (ì†ë„ ìµœì í™”)"""
    try:
        # ë¹ ë¥¸ ëª¨ë“œ ì‹œ ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
        if fast_mode:
            stuff_prompt = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{text}

**ğŸ“„ ë¬¸ì„œ ìœ í˜•:** 
**ğŸ“ í•µì‹¬ ë‚´ìš©:** 
**âš ï¸ ì¤‘ìš” ì •ë³´:** 

ìš”ì•½:"""
        else:
            stuff_prompt = """
            ë‹¤ìŒ ë¬¸ì„œ ì „ì²´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

            {text}

            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

            **ğŸ“„ ë¬¸ì„œ ì •ë³´**
            - ë¬¸ì„œ ìœ í˜•ê³¼ ì£¼ì œ

            **ğŸ“ í•µì‹¬ ë‚´ìš©**
            - ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½ (4-5ê°œ)

            **âš ï¸ ì£¼ìš” í¬ì¸íŠ¸**
            - ë‚ ì§œ, ê¸ˆì•¡, ì¡°ê±´ ë“± ì¤‘ìš” ì •ë³´

            **ğŸ’¡ íŠ¹ì´ì‚¬í•­**
            - ì£¼ëª©í•  ë§Œí•œ ë‚´ìš© (ìˆëŠ” ê²½ìš°ë§Œ)

            ì¢…í•© ìš”ì•½:
            """
        
        prompt = PromptTemplate(template=stuff_prompt, input_variables=["text"])
        
        # Stuff ì²´ì¸ ìƒì„±
        chain = load_summarize_chain(
            llm=llm, 
            chain_type="stuff", 
            prompt=prompt
        )
        
        result = chain.invoke({"input_documents": documents})
        return result["output_text"]
        
    except Exception as e:
        print(f"Stuff ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
        raise e

def _generate_summary_parallel_grouped_method(documents, llm, fast_mode=True):
    """ì¤‘ê°„ í¬ê¸° ë¬¸ì„œë¥¼ ìœ„í•œ ë³‘ë ¬ ê·¸ë£¹í™” ë°©ì‹ (ì†ë„ ìµœì í™”)"""
    try:
        # ë¹ ë¥¸ ëª¨ë“œ ì‹œ ë” í° ê·¸ë£¹ í¬ê¸° ì‚¬ìš©
        group_size = 6 if fast_mode else 4
        groups = [documents[i:i + group_size] for i in range(0, len(documents), group_size)]
        
        print(f"{len(groups)}ê°œ ê·¸ë£¹ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ ì¤‘...")
        
        def process_group(group_data):
            group_idx, group = group_data
            try:
                print(f"ê·¸ë£¹ {group_idx+1} ì²˜ë¦¬ ì¤‘...")
                
                # ê·¸ë£¹ ë‚´ í…ìŠ¤íŠ¸ ê²°í•©
                combined_text = "\n\n".join([doc.page_content for doc in group])
                
                # ë¹ ë¥¸ ëª¨ë“œìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
                if fast_mode:
                    prompt = f"ë‹¤ìŒ ë¬¸ì„œ ë¶€ë¶„ì˜ í•µì‹¬ë§Œ ìš”ì•½: {combined_text[:2000]}..." if len(combined_text) > 2000 else f"ë‹¤ìŒ ë¬¸ì„œ ë¶€ë¶„ì˜ í•µì‹¬ë§Œ ìš”ì•½: {combined_text}"
                else:
                    prompt = f"""
                    ë‹¤ìŒ ë¬¸ì„œ ë¶€ë¶„ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:
                    
                    {combined_text}
                    
                    í•µì‹¬ ìš”ì•½:
                    """
                
                summary = llm.invoke(prompt).content
                return group_idx, summary
                
            except Exception as e:
                print(f"ê·¸ë£¹ {group_idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                return group_idx, f"ê·¸ë£¹ {group_idx+1} ìš”ì•½ ì‹¤íŒ¨"
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
        group_summaries = [None] * len(groups)
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        max_workers = min(4, len(groups))  # ìµœëŒ€ 4ê°œ ìŠ¤ë ˆë“œ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ê° ê·¸ë£¹ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            future_to_group = {
                executor.submit(process_group, (i, group)): i 
                for i, group in enumerate(groups)
            }
            
            # ì™„ë£Œëœ ê²ƒë¶€í„° ê²°ê³¼ ì²˜ë¦¬
            for future in as_completed(future_to_group):
                try:
                    group_idx, summary = future.result()
                    group_summaries[group_idx] = summary
                except Exception as e:
                    group_idx = future_to_group[future]
                    print(f"ê·¸ë£¹ {group_idx+1} ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    group_summaries[group_idx] = f"ê·¸ë£¹ {group_idx+1} ì²˜ë¦¬ ì‹¤íŒ¨"
        
        # None ê°’ ì œê±° (ì‹¤íŒ¨í•œ ê·¸ë£¹)
        valid_summaries = [s for s in group_summaries if s and "ì‹¤íŒ¨" not in s]
        
        # ê·¸ë£¹ ìš”ì•½ë“¤ì„ ì¢…í•©
        print("ê·¸ë£¹ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ëŠ” ì¤‘...")
        
        if fast_mode:
            # ë¹ ë¥¸ ëª¨ë“œ: ê°„ë‹¨í•œ ì¢…í•©
            final_prompt = f"""ë‹¤ìŒ ìš”ì•½ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ì£¼ì„¸ìš”:

{chr(10).join(valid_summaries)}

**ğŸ“„ ë¬¸ì„œ ìœ í˜•:** 
**ğŸ“ ì£¼ìš” ë‚´ìš©:** 
**âš ï¸ ì¤‘ìš” ì •ë³´:** 

ì¢…í•© ìš”ì•½:"""
        else:
            # ì •í™• ëª¨ë“œ: ìƒì„¸í•œ ì¢…í•©
            final_prompt = f"""
            ë‹¤ìŒì€ ë¬¸ì„œì˜ ê° ë¶€ë¶„ë³„ ìš”ì•½ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ì¢…í•©í•´ì„œ ì™„ì „í•œ ë¬¸ì„œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

            {chr(10).join([f"ë¶€ë¶„ {i+1}: {summary}" for i, summary in enumerate(valid_summaries)])}

            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¢…í•© ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

            **ğŸ“„ ë¬¸ì„œ ì •ë³´**
            - ë¬¸ì„œ ìœ í˜•ê³¼ ì£¼ì œ

            **ğŸ“ í•µì‹¬ ë‚´ìš©**
            - ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½

            **âš ï¸ ì£¼ìš” í¬ì¸íŠ¸**
            - ë‚ ì§œ, ê¸ˆì•¡, ì¡°ê±´ ë“± ì¤‘ìš” ì •ë³´

            **ğŸ’¡ íŠ¹ì´ì‚¬í•­**
            - ì£¼ëª©í•  ë§Œí•œ ë‚´ìš©

            ì¢…í•© ìš”ì•½:
            """
        
        final_summary = llm.invoke(final_prompt).content
        return final_summary
        
    except Exception as e:
        print(f"ë³‘ë ¬ ê·¸ë£¹í™” ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
        raise e

def _generate_summary_grouped_method(documents, llm):
    """ì¤‘ê°„ í¬ê¸° ë¬¸ì„œë¥¼ ìœ„í•œ ê·¸ë£¹í™” ë°©ì‹"""
    try:
        # ì²­í¬ë¥¼ 3-4ê°œì”© ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        group_size = 4
        groups = [documents[i:i + group_size] for i in range(0, len(documents), group_size)]
        
        print(f"{len(groups)}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬...")
        
        # ê° ê·¸ë£¹ë³„ ìš”ì•½ ìƒì„±
        group_summaries = []
        for i, group in enumerate(groups):
            print(f"ê·¸ë£¹ {i+1}/{len(groups)} ì²˜ë¦¬ ì¤‘...")
            
            # ê·¸ë£¹ ë‚´ í…ìŠ¤íŠ¸ ê²°í•©
            combined_text = "\n\n".join([doc.page_content for doc in group])
            
            # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
            simple_prompt = f"""
            ë‹¤ìŒ ë¬¸ì„œ ë¶€ë¶„ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {combined_text}
            
            í•µì‹¬ ìš”ì•½:
            """
            
            summary = llm.invoke(simple_prompt).content
            group_summaries.append(summary)
        
        # ê·¸ë£¹ ìš”ì•½ë“¤ì„ ì¢…í•©
        print("ê·¸ë£¹ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ëŠ” ì¤‘...")
        final_prompt = f"""
        ë‹¤ìŒì€ ë¬¸ì„œì˜ ê° ë¶€ë¶„ë³„ ìš”ì•½ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ì¢…í•©í•´ì„œ ì™„ì „í•œ ë¬¸ì„œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        {chr(10).join([f"ë¶€ë¶„ {i+1}: {summary}" for i, summary in enumerate(group_summaries)])}

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¢…í•© ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        **ğŸ“„ ë¬¸ì„œ ì •ë³´**
        - ë¬¸ì„œ ìœ í˜•ê³¼ ì£¼ì œ

        **ğŸ“ í•µì‹¬ ë‚´ìš©**
        - ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš© ìš”ì•½

        **âš ï¸ ì£¼ìš” í¬ì¸íŠ¸**
        - ë‚ ì§œ, ê¸ˆì•¡, ì¡°ê±´ ë“± ì¤‘ìš” ì •ë³´

        **ğŸ’¡ íŠ¹ì´ì‚¬í•­**
        - ì£¼ëª©í•  ë§Œí•œ ë‚´ìš©

        ì¢…í•© ìš”ì•½:
        """
        
        final_summary = llm.invoke(final_prompt).content
        return final_summary
        
    except Exception as e:
        print(f"ê·¸ë£¹í™” ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
        raise e

def _generate_summary_smart_sampling_method(documents, llm, fast_mode=True):
    """í° ë¬¸ì„œë¥¼ ìœ„í•œ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ë°©ì‹ (ì†ë„ ìµœì í™”)"""
    try:
        # ë¹ ë¥¸ ëª¨ë“œì—ì„œëŠ” ë” ê³µê²©ì ìœ¼ë¡œ ìƒ˜í”Œë§
        total_docs = len(documents)
        sample_size = min(12 if fast_mode else 20, total_docs)
        
        # ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œ ì„ íƒ ì „ëµ
        selected_docs = []
        
        # ë¬¸ì„œ ê¸¸ì´ ê¸°ë°˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        doc_scores = []
        for i, doc in enumerate(documents):
            # ìœ„ì¹˜ ì ìˆ˜ (ì•, ë’¤ê°€ ì¤‘ìš”)
            position_score = 1.0 if i < 3 or i >= total_docs - 3 else 0.5
            
            # ë‚´ìš© ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ì§€ë„ ê¸¸ì§€ë„ ì•Šì€ ê²ƒ)
            length_score = min(len(doc.page_content) / 1000, 1.0)
            
            # í‚¤ì›Œë“œ ì ìˆ˜ (ì¤‘ìš”í•œ í‚¤ì›Œë“œ í¬í•¨ ì‹œ ê°€ì )
            important_keywords = ['ê³„ì•½', 'ì¡°ê±´', 'ë‚ ì§œ', 'ê¸ˆì•¡', 'ì˜ë¬´', 'ê¶Œë¦¬', 'ì¡°í•­', 'ë²•ë¥ ', 'ê·œì •']
            keyword_score = sum(1 for keyword in important_keywords if keyword in doc.page_content) / len(important_keywords)
            
            total_score = position_score + length_score + keyword_score
            doc_scores.append((i, doc, total_score))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë¬¸ì„œë“¤ ì„ íƒ
        doc_scores.sort(key=lambda x: x[2], reverse=True)
        selected_docs = [doc for _, doc, _ in doc_scores[:sample_size]]
        
        print(f"ì „ì²´ {total_docs}ê°œ ì²­í¬ ì¤‘ ìƒìœ„ {len(selected_docs)}ê°œ ìŠ¤ë§ˆíŠ¸ ì„ ë³„í•˜ì—¬ ìš”ì•½...")
        
        # ì„ ë³„ëœ ë¬¸ì„œë“¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì ìš©
        return _generate_summary_parallel_grouped_method(selected_docs, llm, fast_mode)
        
    except Exception as e:
        print(f"ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
        raise e

def _generate_summary_sampling_method(documents, llm):
    """í° ë¬¸ì„œë¥¼ ìœ„í•œ ìƒ˜í”Œë§ ë°©ì‹ (ê¸°ì¡´ ë°©ì‹ - í˜¸í™˜ì„± ìœ ì§€)"""
    try:
        # ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ì²­í¬ë“¤ë§Œ ìƒ˜í”Œë§ (ì•, ì¤‘ê°„, ë’¤ + ëœë¤)
        total_docs = len(documents)
        sample_size = min(20, total_docs)  # ìµœëŒ€ 20ê°œ ì²­í¬ë§Œ ì‚¬ìš©
        
        # ìƒ˜í”Œ ì„ íƒ ì „ëµ
        selected_docs = []
        
        # ì•ë¶€ë¶„ (ë¬¸ì„œ ì‹œì‘)
        selected_docs.extend(documents[:3])
        
        # ì¤‘ê°„ë¶€ë¶„
        mid_start = total_docs // 3
        mid_end = mid_start + 3
        selected_docs.extend(documents[mid_start:mid_end])
        
        # ë’·ë¶€ë¶„ (ë¬¸ì„œ ë)
        selected_docs.extend(documents[-3:])
        
        # ë‚˜ë¨¸ì§€ëŠ” ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
        remaining_count = sample_size - len(selected_docs)
        if remaining_count > 0:
            import random
            remaining_docs = [doc for doc in documents if doc not in selected_docs]
            if remaining_docs:
                sampled = random.sample(remaining_docs, min(remaining_count, len(remaining_docs)))
                selected_docs.extend(sampled)
        
        print(f"ì „ì²´ {total_docs}ê°œ ì²­í¬ ì¤‘ {len(selected_docs)}ê°œ ì„ ë³„í•˜ì—¬ ìš”ì•½...")
        
        # ì„ ë³„ëœ ë¬¸ì„œë“¤ë¡œ ê·¸ë£¹í™” ë°©ì‹ ì ìš©
        return _generate_summary_grouped_method(selected_docs, llm)
        
    except Exception as e:
        print(f"ìƒ˜í”Œë§ ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
        raise e

def _generate_simple_summary(documents, llm):
    """ìµœí›„ì˜ í´ë°± - ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹"""
    try:
        # ì²« 10ê°œ ì²­í¬ë§Œ ì‚¬ìš©í•´ì„œ ê°„ë‹¨ ìš”ì•½
        sample_docs = documents[:10]
        combined_text = "\n\n".join([doc.page_content for doc in sample_docs])
        
        simple_prompt = f"""
        ë‹¤ìŒ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

        {combined_text}

        **ğŸ“„ ë¬¸ì„œ ìš”ì•½:**
        - ë¬¸ì„œ ìœ í˜•:
        - ì£¼ìš” ë‚´ìš©:
        - ì¤‘ìš” ì •ë³´:

        ìš”ì•½:
        """
        
        result = llm.invoke(simple_prompt)
        return result.content
        
    except Exception as e:
        return f"âŒ ë¬¸ì„œ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ê¸°ì¡´ í•¨ìˆ˜ë„ ìœ ì§€ (í˜¸í™˜ì„±ì„ ìœ„í•´)
def generate_document_summary(vectorstore, api_key):
    """ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì¢…í•©ì ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°©ì‹)"""
    try:
        # LLM ì„¤ì • (ìš”ì•½ìš©)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.1
        )
        
        # ë¬¸ì„œ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        summary_prompt = """
        ì œê³µëœ ë¬¸ì„œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

        **ğŸ“„ ë¬¸ì„œ ì •ë³´**
        - ë¬¸ì„œ ìœ í˜•ê³¼ ì£¼ì œ

        **ğŸ“ í•µì‹¬ ë‚´ìš©**
        - ì¤‘ìš”í•œ ë‚´ìš© 3-4ê°œ ìš”ì•½

        **âš ï¸ ì£¼ìš” í¬ì¸íŠ¸**
        - ë‚ ì§œ, ê¸ˆì•¡, ì¡°ê±´ ë“± ì¤‘ìš” ì •ë³´

        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """
        
        SUMMARY_PROMPT = PromptTemplate(
            template=summary_prompt,
            input_variables=["context"]
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰ ë° ìš”ì•½
        retriever = vectorstore.as_retriever(search_kwargs={"k": 8})  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        
        # ê´€ë ¨ ë¬¸ì„œë“¤ ê²€ìƒ‰
        docs = retriever.get_relevant_documents("ë¬¸ì„œ ì „ì²´ ë‚´ìš© ìš”ì•½")
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ìš”ì•½ ìƒì„±
        formatted_prompt = SUMMARY_PROMPT.format(context=context)
        response = llm.invoke(formatted_prompt)
        
        return response.content
        
    except Exception as e:
        error_msg = f"ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return f"âŒ ë¬¸ì„œ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ì¶”ì²œ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ ì¶”ê°€
def generate_recommended_questions(vectorstore, api_key):
    """ë¬¸ì„œ ì´í•´ë¥¼ ìœ„í•œ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.1
        )
        
        # ì¶”ì²œ ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
        questions_prompt = """
        ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ë¬¸ì„œë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” í•µì‹¬ ì§ˆë¬¸ë“¤ì„ 5-7ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì´ì–´ì•¼ í•˜ë©°, ë¬¸ì„œì˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.
        
        ê° ì§ˆë¬¸ì€ í•œ ì¤„ì”© ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        - ì§ˆë¬¸ ë‚´ìš©
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """
        
        QUESTIONS_PROMPT = PromptTemplate(
            template=questions_prompt,
            input_variables=["context"]
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰
        retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
        docs = retriever.get_relevant_documents("ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš”í•œ ì •ë³´")
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ì§ˆë¬¸ ìƒì„±
        formatted_prompt = QUESTIONS_PROMPT.format(context=context)
        response = llm.invoke(formatted_prompt)
        
        # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        questions_text = response.content
        questions = []
        
        for line in questions_text.split('\n'):
            line = line.strip()
            if line.startswith('-') and len(line) > 3:
                question = line[1:].strip()
                if question and '?' in question:
                    questions.append(question)
        
        return questions[:7]  # ìµœëŒ€ 7ê°œ ì§ˆë¬¸ë§Œ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return []

# ì¶”ê°€ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_additional_questions(vectorstore, api_key, existing_questions):
    """ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì¶”ê°€ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.2  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ temperature ì•½ê°„ ì¦ê°€
        )
        
        # ê¸°ì¡´ ì§ˆë¬¸ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        existing_questions_text = "\n".join([f"- {q}" for q in existing_questions])
        
        # ì¶”ê°€ ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
        additional_questions_prompt = """
        ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ì ì¸ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ 5-6ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì•„ë˜ ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì„¸ë¶€ì‚¬í•­ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        
        **ê¸°ì¡´ ì§ˆë¬¸ë“¤ (ì¤‘ë³µ ê¸ˆì§€):**
        {existing_questions}
        
        **ìƒˆë¡œìš´ ì§ˆë¬¸ ìƒì„± ì¡°ê±´:**
        - ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©ì´ì–´ì•¼ í•¨
        - ë¬¸ì„œì˜ ë‹¤ë¥¸ ì¸¡ë©´ì´ë‚˜ ì„¸ë¶€ì‚¬í•­ì— ì´ˆì 
        - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸
        - ë¬¸ì„œ ë‚´ìš©ì„ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
        
        ê° ì§ˆë¬¸ì€ í•œ ì¤„ì”© ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        - ì§ˆë¬¸ ë‚´ìš©
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """
        
        ADDITIONAL_QUESTIONS_PROMPT = PromptTemplate(
            template=additional_questions_prompt,
            input_variables=["existing_questions", "context"]
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰ (ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
        docs = retriever.get_relevant_documents("ë¬¸ì„œì˜ ì„¸ë¶€ì‚¬í•­ê³¼ ì¶”ê°€ ì •ë³´")
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
        formatted_prompt = ADDITIONAL_QUESTIONS_PROMPT.format(
            existing_questions=existing_questions_text,
            context=context
        )
        response = llm.invoke(formatted_prompt)
        
        # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        questions_text = response.content
        new_questions = []
        
        for line in questions_text.split('\n'):
            line = line.strip()
            if line.startswith('-') and len(line) > 3:
                question = line[1:].strip()
                if question and '?' in question:
                    # ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì¤‘ë³µ ì²´í¬ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¹„êµ)
                    is_duplicate = False
                    for existing_q in existing_questions:
                        # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ 70% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                        existing_words = set(existing_q.lower().split())
                        new_words = set(question.lower().split())
                        if len(existing_words & new_words) / max(len(existing_words), len(new_words)) > 0.7:
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        new_questions.append(question)
        
        return new_questions[:6]  # ìµœëŒ€ 6ê°œ ì¶”ê°€ ì§ˆë¬¸ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return []

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None

if 'document_splits' not in st.session_state:
    st.session_state.document_splits = None

if 'current_file' not in st.session_state:
    st.session_state.current_file = None

if 'document_summary' not in st.session_state:
    st.session_state.document_summary = None

if 'recommended_questions' not in st.session_state:
    st.session_state.recommended_questions = []

# íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file:
    # ìƒˆ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if st.session_state.current_file != uploaded_file.name:
        st.session_state.current_file = uploaded_file.name
        st.session_state.vectorstore = None  # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        st.session_state.document_splits = None  # ê¸°ì¡´ ë¬¸ì„œ ë¶„í•  ì´ˆê¸°í™”
        st.session_state.messages = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        st.session_state.document_summary = None  # ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”
        st.session_state.recommended_questions = []  # ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™”
        
        # íŒŒì¼ ì €ì¥ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì„ë² ë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
            temp_file_path = save_uploaded_file(uploaded_file)
            if temp_file_path:
                # vectorstoreì™€ ë¬¸ì„œ ë¶„í•  ì •ë³´ë¥¼ í•¨ê»˜ ë°›ìŒ
                vectorstore, document_splits = initialize_rag_system(temp_file_path)
                st.session_state.vectorstore = vectorstore
                st.session_state.document_splits = document_splits
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (í¬ë¡œìŠ¤ í”Œë«í¼)
                try:
                    os.unlink(temp_file_path)
                    print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ë¨: {temp_file_path}")
                except Exception as e:
                    print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
                
                if st.session_state.vectorstore and st.session_state.document_splits:
                    st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ë¬¸ì„œ ìš”ì•½ ë° ì¶”ì²œ ì§ˆë¬¸ ìë™ ìƒì„±
                    if openai_api_key:
                        # ì„ íƒëœ ëª¨ë“œì— ë”°ë¥¸ ìŠ¤í”¼ë„ˆ ë©”ì‹œì§€
                        fast_mode = st.session_state.get('summary_fast_mode', True)
                        spinner_msg = f"ğŸ“ {'ë¹ ë¥¸' if fast_mode else 'ì •í™•'} ëª¨ë“œë¡œ ë¬¸ì„œ ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                        
                        with st.spinner(spinner_msg):
                            # ì„ íƒëœ ëª¨ë“œë¡œ ìš”ì•½ ìƒì„±
                            summary = generate_document_summary_improved(
                                st.session_state.document_splits, 
                                openai_api_key,
                                fast_mode=fast_mode
                            )
                            st.session_state.document_summary = summary
                            
                        with st.spinner("ğŸ¤” í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            questions = generate_recommended_questions(st.session_state.vectorstore, openai_api_key)
                            st.session_state.recommended_questions = questions
                            
                        mode_text = "ë¹ ë¥¸" if fast_mode else "ì •í™•"
                        st.success(f"âœ… {mode_text} ëª¨ë“œë¡œ ë¬¸ì„œ ìš”ì•½ê³¼ í•µì‹¬ ì§ˆë¬¸ë“¤ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    else:
                        st.warning("âš ï¸ API í‚¤ê°€ ì—†ì–´ ë¬¸ì„œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ë¬¸ì„œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
else:
    # íŒŒì¼ì´ ì‚­ì œëœ ê²½ìš° ì²˜ë¦¬
    if st.session_state.current_file is not None:
        # ì•ˆì „í•œ ChromaDB í´ë” ì‚­ì œ
        with st.spinner("ğŸ—‘ï¸ ê¸°ì¡´ ë¬¸ì„œ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            success = cleanup_all_chromadb()
            if success:
                st.info("ğŸ—‘ï¸ ê¸°ì¡´ ë¬¸ì„œ ë°ì´í„°ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("âš ï¸ ë°ì´í„° ì •ë¦¬ê°€ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì•± ì¬ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ ì •ë¦¬ë©ë‹ˆë‹¤.")
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.current_file = None
        st.session_state.vectorstore = None
        st.session_state.document_splits = None
        st.session_state.messages = []
        st.session_state.document_summary = None
        st.session_state.recommended_questions = []

# íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œ ì•ˆë‚´
if not uploaded_file:
    st.info("ğŸ“ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
    st.markdown("""
    ### ğŸ“‹ ì‚¬ìš© ë°©ë²•
    1. **íŒŒì¼ ì—…ë¡œë“œ**: ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ ì—…ë¡œë“œ
    2. **ë¬¸ì„œ ë¶„ì„**: ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ìë™ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤
    3. **ì§ˆë¬¸í•˜ê¸°**: ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
    
    ### ğŸ“„ ì§€ì› íŒŒì¼ í˜•ì‹
    - PDF íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤
    - í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤
    """)
else:
    # ë¬¸ì„œ ìš”ì•½ í‘œì‹œ (íŒŒì¼ì´ ì—…ë¡œë“œë˜ê³  ìš”ì•½ì´ ìˆì„ ë•Œ)
    if st.session_state.document_summary:
        st.markdown("---")
        st.markdown("## ğŸ“„ ë¬¸ì„œ ìš”ì•½")
        
        # ìš”ì•½ì„ ì˜ˆì˜ê²Œ í‘œì‹œ
        with st.container():
            st.markdown(st.session_state.document_summary)
        
        # ì¶”ì²œ ì§ˆë¬¸ ë²„íŠ¼ë“¤ í‘œì‹œ
        if st.session_state.recommended_questions:
            st.markdown("---")
            st.markdown("## ğŸ¤” í•µì‹¬ ì§ˆë¬¸ë“¤")
            st.info("ğŸ’¡ ì•„ë˜ ì§ˆë¬¸ë“¤ì„ í´ë¦­í•˜ë©´ ë°”ë¡œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            
            # ì§ˆë¬¸ ë²„íŠ¼ë“¤ì„ 2ì—´ë¡œ ë°°ì¹˜
            cols = st.columns(2)
            for idx, question in enumerate(st.session_state.recommended_questions):
                col_idx = idx % 2
                with cols[col_idx]:
                    # ì§ˆë¬¸ì„ ë²„íŠ¼ìœ¼ë¡œ í‘œì‹œ (í‚¤ ê°’ì— ì¸ë±ìŠ¤ ì¶”ê°€ë¡œ ì¤‘ë³µ ë°©ì§€)
                    if st.button(
                        question, 
                        key=f"question_btn_{idx}",
                        help="í´ë¦­í•˜ë©´ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤",
                        use_container_width=True
                    ):
                        # ë²„íŠ¼ì´ í´ë¦­ë˜ë©´ í•´ë‹¹ ì§ˆë¬¸ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
                        if st.session_state.vectorstore and openai_api_key:
                            # ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ì¶”ê°€
                            st.session_state.messages.append({"role": "user", "content": question})
                            
                            # ë‹µë³€ ìƒì„±
                            with st.spinner("ğŸ¤– ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                # ì„ì‹œ ì»¨í…Œì´ë„ˆ ìƒì„± (ì‹¤ì œë¡œ í‘œì‹œí•˜ì§€ ì•Šê¸° ìœ„í•´)
                                temp_container = st.empty()
                                ai_response = get_rag_response_streaming(
                                    question,
                                    st.session_state.vectorstore,
                                    openai_api_key,
                                    temp_container
                                )
                                temp_container.empty()  # ì„ì‹œ ì»¨í…Œì´ë„ˆ ì •ë¦¬
                            
                            # AI ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
                            st.session_state.messages.append({"role": "assistant", "content": ai_response})
                            
                            # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ì—…ë°ì´íŠ¸
                            st.rerun()
            
            # ë” ë§ì€ ì§ˆë¬¸ ìƒì„± ë²„íŠ¼ ì¶”ê°€
            st.markdown("---")
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button(
                    "ğŸ”„ ë” ë§ì€ ì§ˆë¬¸ ìƒì„±í•˜ê¸°",
                    key="generate_more_questions",
                    help="ì¶”ê°€ì ì¸ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤",
                    use_container_width=True,
                    type="secondary"
                ):
                    if st.session_state.vectorstore and openai_api_key:
                        with st.spinner("ğŸ¤” ì¶”ê°€ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            # ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ ìƒì„±
                            new_questions = generate_additional_questions(
                                st.session_state.vectorstore, 
                                openai_api_key, 
                                st.session_state.recommended_questions
                            )
                            
                            if new_questions:
                                # ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ì„ ê¸°ì¡´ ì§ˆë¬¸ë“¤ì— ì¶”ê°€
                                st.session_state.recommended_questions.extend(new_questions)
                                st.success(f"âœ… {len(new_questions)}ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ë“¤ í‘œì‹œ
                            else:
                                st.warning("âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    else:
                        st.error("âŒ ë¬¸ì„œê°€ ë¶„ì„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        st.markdown("### ğŸ’¬ ì§ì ‘ ì§ˆë¬¸í•˜ê¸°")
        st.info("ğŸ“ ìœ„ ì§ˆë¬¸ë“¤ ì™¸ì—ë„ ë¬¸ì„œì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    
    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_question := st.chat_input(placeholder="ì—…ë¡œë“œëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”... ğŸ’¬"):
        # API í‚¤ í™•ì¸
        if not openai_api_key:
            st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”! .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        elif st.session_state.vectorstore is None:
            st.error("ë¬¸ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(user_question)
            st.session_state.messages.append({"role": "user", "content": user_question})

            # AI ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
            with st.chat_message("assistant"):
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±
                message_container = st.empty()
                
                # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                ai_response = get_rag_response_streaming(
                    user_question, 
                    st.session_state.vectorstore, 
                    openai_api_key,
                    message_container
                )
                
            st.session_state.messages.append({"role": "assistant", "content": ai_response})

# í•˜ë‹¨ì— ì‚¬ìš© íŒ ì¶”ê°€
if uploaded_file:
    st.divider()
    with st.expander("ğŸ’¡ ì‚¬ìš© íŒ"):
        st.markdown(f"""
        **í˜„ì¬ ë¶„ì„ ì¤‘ì¸ íŒŒì¼: {uploaded_file.name}**
        **ì‹œìŠ¤í…œ í™˜ê²½: {SYSTEM_OS.title()}** {'(ë°°í¬í™˜ê²½)' if IS_DEPLOYMENT else '(ë¡œì»¬í™˜ê²½)'}
        
        **ì´ ì±—ë´‡ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?**
        - ğŸ“„ ì—…ë¡œë“œëœ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
        - ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
        - âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤
        
        **ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ íŒ:**
        - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
        - ë¬¸ì„œì˜ íŠ¹ì • ë¶€ë¶„ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
        - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """)

    # ë””ë²„ê¹…ìš© (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
    if st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ"):
        st.json({
            "ë©”ì‹œì§€ ê°œìˆ˜": len(st.session_state.messages),
            "OS": SYSTEM_OS,
            "ë°°í¬í™˜ê²½": IS_DEPLOYMENT,
            "Python ë²„ì „": sys.version
        })
        with st.expander("ì „ì²´ ëŒ€í™” ë‚´ì—­"):
            st.json(st.session_state.messages)