import os
import platform
import sys

# protobuf í˜¸í™˜ì„±ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“  importë³´ë‹¤ ë¨¼ì €!)
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

# sqlite3 íŒ¨ì¹˜ (ë°°í¬ í™˜ê²½ ê³ ë ¤)
try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    print("<<<<< sqlite3 patched with pysqlite3 >>>>>")
except ImportError:
    # pysqlite3ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ sqlite3 ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ì •ìƒ)
    print("<<<<< using default sqlite3 >>>>>")

print(f"<<<<< app.py IS BEING LOADED on {platform.system()} >>>>>")

import streamlit as st
import tempfile
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from typing import Dict, List, Any
import time
import uuid
import subprocess
import shutil
import glob
import gc
from pathlib import Path

# from streamlit_extras.buy_me_a_coffee import button

# ìˆ˜ë™ìœ¼ë¡œ Buy Me a Coffee ë²„íŠ¼ êµ¬í˜„
def add_buy_me_coffee_button():
    st.markdown(
        """
        <style>
        .coffee-button {
            text-align: left;
            margin-bottom: 20px;
        }
        .coffee-button img {
            height: 40px !important;
            width: 144px !important;
            max-height: 40px !important;
            max-width: 144px !important;
            object-fit: contain !important;
            border: none !important;
        }
        </style>
        <div class="coffee-button">
            <a href="https://www.buymeacoffee.com/ehtjd33e" target="_blank">
                <img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" 
                     alt="Buy Me A Coffee">
            </a>
        </div>
        """,
        unsafe_allow_html=True
    )

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="ë²•ë¥ ê°€ ì±—ë´‡", page_icon=":books:", layout="wide")

# ë²„íŠ¼ ì¶”ê°€ (ì œëª© ë°”ë¡œ ìœ„)
add_buy_me_coffee_button()

st.title("ğŸ“š ë²•ë¥ ê°€ ì±—ë´‡")
st.caption("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€ë°›ìœ¼ì„¸ìš”")

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")

# OS ê°ì§€ ë° í”Œë«í¼ë³„ ì„¤ì •
SYSTEM_OS = platform.system().lower()
IS_WINDOWS = SYSTEM_OS == 'windows'
IS_LINUX = SYSTEM_OS == 'linux'
IS_MACOS = SYSTEM_OS == 'darwin'

# ë°°í¬ í™˜ê²½ ê°ì§€ (Streamlit Cloud, Heroku ë“±)
IS_DEPLOYMENT = any([
    os.getenv('STREAMLIT_CLOUD'),
    os.getenv('HEROKU'),
    os.getenv('RAILWAY'),
    os.getenv('RENDER'),
    '/app' in os.getcwd(),  # ì¼ë°˜ì ì¸ ì»¨í…Œì´ë„ˆ ê²½ë¡œ
    '/home/appuser' in os.getcwd()  # Streamlit Cloud ê²½ë¡œ
])

print(f"ì‹œìŠ¤í…œ ì •ë³´: OS={SYSTEM_OS}, ë°°í¬í™˜ê²½={IS_DEPLOYMENT}")

# ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–‹")  # ì»¤ì„œ íš¨ê³¼
        time.sleep(0.01)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.container.markdown(self.text)  # ìµœì¢… í…ìŠ¤íŠ¸ (ì»¤ì„œ ì œê±°)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ë²•ë¥  ë¬¸ì„œë‚˜ ê´€ë ¨ ìë£Œë¥¼ PDF í˜•íƒœë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    st.divider()
    
    st.header("âš™ï¸ ì„¤ì •")
    if not openai_api_key:
        st.error("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        st.info("ğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:\nOPENAI_API_KEY=your_api_key_here")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    if uploaded_file:
        st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œë¨: {uploaded_file.name}")
        st.info(f"ğŸ“„ íŒŒì¼ í¬ê¸°: {uploaded_file.size / 1024:.1f} KB")

# í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ ChromaDB ì‚­ì œ í•¨ìˆ˜
def safe_delete_chromadb_cross_platform(target_dir, max_retries=3, delay=1):
    """ëª¨ë“  OSì—ì„œ ì•ˆì „í•˜ê²Œ ChromaDB í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    
    # ë¨¼ì € vectorstore ê°ì²´ í•´ì œ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    if 'vectorstore' in st.session_state:
        st.session_state.vectorstore = None
    gc.collect()
    
    if not os.path.exists(target_dir):
        return True
    
    # Path ê°ì²´ ì‚¬ìš©ìœ¼ë¡œ í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„± ê°œì„ 
    target_path = Path(target_dir)
    temp_name = f"{target_dir}_deleted_{uuid.uuid4().hex[:8]}"
    temp_path = Path(temp_name)
    
    try:
        # 1ë‹¨ê³„: í´ë” ì´ë¦„ ë³€ê²½ (ëª¨ë“  OSì—ì„œ ì•ˆì „)
        target_path.rename(temp_path)
        print(f"ChromaDB ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½: {target_dir} -> {temp_name}")
        
        # 2ë‹¨ê³„: í”Œë«í¼ë³„ ì‚­ì œ ì „ëµ
        success = False
        
        for attempt in range(max_retries):
            try:
                time.sleep(delay)
                
                if IS_WINDOWS and not IS_DEPLOYMENT:
                    # Windows ë¡œì»¬ í™˜ê²½: ê°•í™”ëœ ì‚­ì œ ë°©ë²•
                    success = _windows_force_delete(temp_path)
                else:
                    # Linux/macOS ë˜ëŠ” ë°°í¬ í™˜ê²½: í‘œì¤€ ë°©ë²•
                    success = _standard_delete(temp_path)
                
                if success:
                    print(f"ChromaDB ë””ë ‰í† ë¦¬ ì‚­ì œ ì„±ê³µ: {temp_name}")
                    return True
                    
            except Exception as e:
                print(f"ì‚­ì œ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {str(e)}")
                if attempt < max_retries - 1:
                    delay *= 2
        
        # ì‚­ì œ ì‹¤íŒ¨ ì‹œì—ë„ ì´ë¦„ ë³€ê²½ì€ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ True ë°˜í™˜
        print(f"ChromaDB ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨, ë‚˜ì¤‘ì— ì •ë¦¬ë¨: {temp_name}")
        return True
        
    except Exception as e:
        print(f"ChromaDB ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return False

def _windows_force_delete(path):
    """Windowsì—ì„œ ê°•í™”ëœ ì‚­ì œ ë°©ë²•"""
    try:
        # ë°©ë²• 1: í‘œì¤€ shutil ì‚­ì œ
        shutil.rmtree(path)
        return True
    except Exception:
        pass
    
    try:
        # ë°©ë²• 2: Windows ì „ìš© robocopy ì‚¬ìš© (robocopyê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if shutil.which("robocopy"):
            empty_dir = Path("./temp_empty_dir")
            empty_dir.mkdir(exist_ok=True)
            
            # robocopyë¡œ ë¹ˆ í´ë”ë¥¼ ë¯¸ëŸ¬ë§í•˜ì—¬ ì‚­ì œ íš¨ê³¼
            result = subprocess.run([
                "robocopy", str(empty_dir), str(path), "/MIR", "/NFL", "/NDL", "/NJH", "/NJS"
            ], capture_output=True, text=True)
            
            # ë¹ˆ í´ë”ë“¤ ì •ë¦¬
            empty_dir.rmdir()
            path.rmdir()
            return True
    except Exception:
        pass
    
    try:
        # ë°©ë²• 3: íŒŒì¼ë³„ ê°œë³„ ì‚­ì œ
        for root, dirs, files in os.walk(path, topdown=False):
            for file in files:
                os.chmod(os.path.join(root, file), 0o777)
                os.remove(os.path.join(root, file))
            for dir in dirs:
                os.rmdir(os.path.join(root, dir))
        os.rmdir(path)
        return True
    except Exception:
        pass
    
    return False

def _standard_delete(path):
    """í‘œì¤€ ì‚­ì œ ë°©ë²• (Linux/macOS/ë°°í¬í™˜ê²½)"""
    try:
        shutil.rmtree(path)
        return True
    except Exception as e:
        print(f"í‘œì¤€ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return False

# ì „ì²´ ChromaDB ì •ë¦¬ í•¨ìˆ˜ (í¬ë¡œìŠ¤ í”Œë«í¼)
def cleanup_all_chromadb():
    """ëª¨ë“  ChromaDB ê´€ë ¨ í´ë”ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ChromaDB ê´€ë ¨ í´ë” ì°¾ê¸°
        current_dir = Path.cwd()
        chroma_patterns = ["chroma_db*", ".chromadb*"]
        
        cleaned_count = 0
        for pattern in chroma_patterns:
            for chroma_path in current_dir.glob(pattern):
                if chroma_path.is_dir():
                    try:
                        success = safe_delete_chromadb_cross_platform(str(chroma_path))
                        if success:
                            cleaned_count += 1
                            print(f"ì •ë¦¬ë¨: {chroma_path}")
                    except Exception as e:
                        print(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ({chroma_path}): {str(e)}")
        
        if cleaned_count > 0:
            print(f"ì´ {cleaned_count}ê°œì˜ ChromaDB í´ë”ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
    except Exception as e:
        print(f"ChromaDB ì „ì²´ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

# ì•ˆì „í•œ ChromaDB ì‚­ì œ í•¨ìˆ˜ (ê¸°ë³¸ í˜¸í™˜)
def safe_delete_chromadb(max_retries=3, delay=1):
    """ChromaDB í´ë”ë¥¼ ì•ˆì „í•˜ê²Œ ì‚­ì œí•©ë‹ˆë‹¤."""
    return safe_delete_chromadb_cross_platform("./chroma_db", max_retries, delay)

# ì•± ì‹œì‘ ì‹œ ChromaDB ì •ë¦¬ (ê°œì„ ëœ ë²„ì „)
if 'app_initialized' not in st.session_state:
    print(f"ì•± ì´ˆê¸°í™” ì‹œì‘ - OS: {SYSTEM_OS}, ë°°í¬í™˜ê²½: {IS_DEPLOYMENT}")
    
    # ì „ì²´ ChromaDB í´ë” ì •ë¦¬
    cleanup_all_chromadb()
    
    st.session_state.app_initialized = True
    print("ì•± ì´ˆê¸°í™” ì™„ë£Œ")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í¬ë¡œìŠ¤ í”Œë«í¼ ê°œì„  ë²„ì „)
@st.cache_resource
def initialize_rag_system(file_path):
    """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘: {file_path}")
        
        # PDF ë¬¸ì„œ ë¡œë”©
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        print(f"ë¬¸ì„œ ë¡œë”© ì™„ë£Œ: {len(documents)}ê°œ í˜ì´ì§€")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬")
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # ê³ ìœ í•œ ChromaDB ë””ë ‰í† ë¦¬ ì‚¬ìš© (ì¶©ëŒ ë°©ì§€)
        timestamp = int(time.time())
        random_id = uuid.uuid4().hex[:8]
        chroma_dir = f"./chroma_db_{timestamp}_{random_id}"
        
        print(f"ìƒˆ ChromaDB ë””ë ‰í† ë¦¬: {chroma_dir}")
        
        # ê¸°ì¡´ chroma_db í´ë”ê°€ ìˆë‹¤ë©´ ì •ë¦¬
        old_chroma_dir = "./chroma_db"
        if os.path.exists(old_chroma_dir):
            try:
                safe_delete_chromadb_cross_platform(old_chroma_dir)
                print(f"ê¸°ì¡´ ChromaDB ë””ë ‰í† ë¦¬ ì •ë¦¬ë¨: {old_chroma_dir}")
            except Exception as e:
                print(f"ê¸°ì¡´ ChromaDB ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")

        # ìƒˆë¡œìš´ ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=chroma_dir
        )
        
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        return vectorstore
        
    except Exception as e:
        error_msg = f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        st.error(error_msg)
        return None

# ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (í¬ë¡œìŠ¤ í”Œë«í¼)
def save_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # í¬ë¡œìŠ¤ í”Œë«í¼ ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix="uploaded_") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            temp_path = tmp_file.name
        
        print(f"ì„ì‹œ íŒŒì¼ ìƒì„±: {temp_path}")
        return temp_path
        
    except Exception as e:
        error_msg = f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        st.error(error_msg)
        return None

# RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
def get_rag_response_streaming(question, vectorstore, api_key, container):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)."""
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = StreamlitCallbackHandler(container)
        
        # LLM ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0,
            streaming=True,
            callbacks=[callback_handler]
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¼ ê²½ìš° ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        response = qa_chain.invoke({"query": question})
        return callback_handler.text  # ìŠ¤íŠ¸ë¦¬ë°ëœ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_msg)
        container.error(error_msg)
        return error_msg

# ë¬¸ì„œ ìš”ì•½ í•¨ìˆ˜ ì¶”ê°€
def generate_document_summary(vectorstore, api_key):
    """ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì¢…í•©ì ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì • (ìš”ì•½ìš©)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.1
        )
        
        # ë¬¸ì„œ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        summary_prompt = """
        ì œê³µëœ ë¬¸ì„œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

        **ğŸ“„ ë¬¸ì„œ ì •ë³´**
        - ë¬¸ì„œ ìœ í˜•ê³¼ ì£¼ì œ

        **ğŸ“ í•µì‹¬ ë‚´ìš©**
        - ì¤‘ìš”í•œ ë‚´ìš© 3-4ê°œ ìš”ì•½

        **âš ï¸ ì£¼ìš” í¬ì¸íŠ¸**
        - ë‚ ì§œ, ê¸ˆì•¡, ì¡°ê±´ ë“± ì¤‘ìš” ì •ë³´

        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """
        
        SUMMARY_PROMPT = PromptTemplate(
            template=summary_prompt,
            input_variables=["context"]
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰ ë° ìš”ì•½
        retriever = vectorstore.as_retriever(search_kwargs={"k": 8})  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        
        # ê´€ë ¨ ë¬¸ì„œë“¤ ê²€ìƒ‰
        docs = retriever.get_relevant_documents("ë¬¸ì„œ ì „ì²´ ë‚´ìš© ìš”ì•½")
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ìš”ì•½ ìƒì„±
        formatted_prompt = SUMMARY_PROMPT.format(context=context)
        response = llm.invoke(formatted_prompt)
        
        return response.content
        
    except Exception as e:
        error_msg = f"ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return f"âŒ ë¬¸ì„œ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ì¶”ì²œ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ ì¶”ê°€
def generate_recommended_questions(vectorstore, api_key):
    """ë¬¸ì„œ ì´í•´ë¥¼ ìœ„í•œ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.1
        )
        
        # ì¶”ì²œ ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
        questions_prompt = """
        ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ë¬¸ì„œë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” í•µì‹¬ ì§ˆë¬¸ë“¤ì„ 5-7ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì´ì–´ì•¼ í•˜ë©°, ë¬¸ì„œì˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.
        
        ê° ì§ˆë¬¸ì€ í•œ ì¤„ì”© ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        - ì§ˆë¬¸ ë‚´ìš©
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """
        
        QUESTIONS_PROMPT = PromptTemplate(
            template=questions_prompt,
            input_variables=["context"]
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰
        retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
        docs = retriever.get_relevant_documents("ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš”í•œ ì •ë³´")
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ì§ˆë¬¸ ìƒì„±
        formatted_prompt = QUESTIONS_PROMPT.format(context=context)
        response = llm.invoke(formatted_prompt)
        
        # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        questions_text = response.content
        questions = []
        
        for line in questions_text.split('\n'):
            line = line.strip()
            if line.startswith('-') and len(line) > 3:
                question = line[1:].strip()
                if question and '?' in question:
                    questions.append(question)
        
        return questions[:7]  # ìµœëŒ€ 7ê°œ ì§ˆë¬¸ë§Œ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return []

# ì¶”ê°€ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_additional_questions(vectorstore, api_key, existing_questions):
    """ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì¶”ê°€ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0.2  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ temperature ì•½ê°„ ì¦ê°€
        )
        
        # ê¸°ì¡´ ì§ˆë¬¸ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        existing_questions_text = "\n".join([f"- {q}" for q in existing_questions])
        
        # ì¶”ê°€ ì§ˆë¬¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
        additional_questions_prompt = """
        ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ì ì¸ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ 5-6ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì•„ë˜ ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì„¸ë¶€ì‚¬í•­ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        
        **ê¸°ì¡´ ì§ˆë¬¸ë“¤ (ì¤‘ë³µ ê¸ˆì§€):**
        {existing_questions}
        
        **ìƒˆë¡œìš´ ì§ˆë¬¸ ìƒì„± ì¡°ê±´:**
        - ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ë‚´ìš©ì´ì–´ì•¼ í•¨
        - ë¬¸ì„œì˜ ë‹¤ë¥¸ ì¸¡ë©´ì´ë‚˜ ì„¸ë¶€ì‚¬í•­ì— ì´ˆì 
        - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸
        - ë¬¸ì„œ ë‚´ìš©ì„ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
        
        ê° ì§ˆë¬¸ì€ í•œ ì¤„ì”© ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        - ì§ˆë¬¸ ë‚´ìš©
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """
        
        ADDITIONAL_QUESTIONS_PROMPT = PromptTemplate(
            template=additional_questions_prompt,
            input_variables=["existing_questions", "context"]
        )
        
        # ë¬¸ì„œ ê²€ìƒ‰ (ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
        docs = retriever.get_relevant_documents("ë¬¸ì„œì˜ ì„¸ë¶€ì‚¬í•­ê³¼ ì¶”ê°€ ì •ë³´")
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
        formatted_prompt = ADDITIONAL_QUESTIONS_PROMPT.format(
            existing_questions=existing_questions_text,
            context=context
        )
        response = llm.invoke(formatted_prompt)
        
        # ì§ˆë¬¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        questions_text = response.content
        new_questions = []
        
        for line in questions_text.split('\n'):
            line = line.strip()
            if line.startswith('-') and len(line) > 3:
                question = line[1:].strip()
                if question and '?' in question:
                    # ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì¤‘ë³µ ì²´í¬ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¹„êµ)
                    is_duplicate = False
                    for existing_q in existing_questions:
                        # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ 70% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                        existing_words = set(existing_q.lower().split())
                        new_words = set(question.lower().split())
                        if len(existing_words & new_words) / max(len(existing_words), len(new_words)) > 0.7:
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        new_questions.append(question)
        
        return new_questions[:6]  # ìµœëŒ€ 6ê°œ ì¶”ê°€ ì§ˆë¬¸ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return []

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None

if 'current_file' not in st.session_state:
    st.session_state.current_file = None

if 'document_summary' not in st.session_state:
    st.session_state.document_summary = None

if 'recommended_questions' not in st.session_state:
    st.session_state.recommended_questions = []

# íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file:
    # ìƒˆ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if st.session_state.current_file != uploaded_file.name:
        st.session_state.current_file = uploaded_file.name
        st.session_state.vectorstore = None  # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        st.session_state.messages = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        st.session_state.document_summary = None  # ë¬¸ì„œ ìš”ì•½ ì´ˆê¸°í™”
        st.session_state.recommended_questions = []  # ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™”
        
        # íŒŒì¼ ì €ì¥ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì„ë² ë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
            temp_file_path = save_uploaded_file(uploaded_file)
            if temp_file_path:
                st.session_state.vectorstore = initialize_rag_system(temp_file_path)
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (í¬ë¡œìŠ¤ í”Œë«í¼)
                try:
                    os.unlink(temp_file_path)
                    print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ë¨: {temp_file_path}")
                except Exception as e:
                    print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
                
                if st.session_state.vectorstore:
                    st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ë¬¸ì„œ ìš”ì•½ ë° ì¶”ì²œ ì§ˆë¬¸ ìë™ ìƒì„±
                    if openai_api_key:
                        with st.spinner("ğŸ“ ë¬¸ì„œ ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            summary = generate_document_summary(st.session_state.vectorstore, openai_api_key)
                            st.session_state.document_summary = summary
                            
                        with st.spinner("ğŸ¤” í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            questions = generate_recommended_questions(st.session_state.vectorstore, openai_api_key)
                            st.session_state.recommended_questions = questions
                            
                        st.success("âœ… ë¬¸ì„œ ìš”ì•½ê³¼ í•µì‹¬ ì§ˆë¬¸ë“¤ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    else:
                        st.warning("âš ï¸ API í‚¤ê°€ ì—†ì–´ ë¬¸ì„œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ë¬¸ì„œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
else:
    # íŒŒì¼ì´ ì‚­ì œëœ ê²½ìš° ì²˜ë¦¬
    if st.session_state.current_file is not None:
        # ì•ˆì „í•œ ChromaDB í´ë” ì‚­ì œ
        with st.spinner("ğŸ—‘ï¸ ê¸°ì¡´ ë¬¸ì„œ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            success = cleanup_all_chromadb()
            if success:
                st.info("ğŸ—‘ï¸ ê¸°ì¡´ ë¬¸ì„œ ë°ì´í„°ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("âš ï¸ ë°ì´í„° ì •ë¦¬ê°€ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì•± ì¬ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ ì •ë¦¬ë©ë‹ˆë‹¤.")
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.current_file = None
        st.session_state.vectorstore = None
        st.session_state.messages = []
        st.session_state.document_summary = None
        st.session_state.recommended_questions = []

# íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œ ì•ˆë‚´
if not uploaded_file:
    st.info("ğŸ“ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
    st.markdown("""
    ### ğŸ“‹ ì‚¬ìš© ë°©ë²•
    1. **íŒŒì¼ ì—…ë¡œë“œ**: ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ ì—…ë¡œë“œ
    2. **ë¬¸ì„œ ë¶„ì„**: ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ìë™ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤
    3. **ì§ˆë¬¸í•˜ê¸°**: ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
    
    ### ğŸ“„ ì§€ì› íŒŒì¼ í˜•ì‹
    - PDF íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤
    - í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤
    """)
else:
    # ë¬¸ì„œ ìš”ì•½ í‘œì‹œ (íŒŒì¼ì´ ì—…ë¡œë“œë˜ê³  ìš”ì•½ì´ ìˆì„ ë•Œ)
    if st.session_state.document_summary:
        st.markdown("---")
        st.markdown("## ğŸ“„ ë¬¸ì„œ ìš”ì•½")
        
        # ìš”ì•½ì„ ì˜ˆì˜ê²Œ í‘œì‹œ
        with st.container():
            st.markdown(st.session_state.document_summary)
        
        # ì¶”ì²œ ì§ˆë¬¸ ë²„íŠ¼ë“¤ í‘œì‹œ
        if st.session_state.recommended_questions:
            st.markdown("---")
            st.markdown("## ğŸ¤” í•µì‹¬ ì§ˆë¬¸ë“¤")
            st.info("ğŸ’¡ ì•„ë˜ ì§ˆë¬¸ë“¤ì„ í´ë¦­í•˜ë©´ ë°”ë¡œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            
            # ì§ˆë¬¸ ë²„íŠ¼ë“¤ì„ 2ì—´ë¡œ ë°°ì¹˜
            cols = st.columns(2)
            for idx, question in enumerate(st.session_state.recommended_questions):
                col_idx = idx % 2
                with cols[col_idx]:
                    # ì§ˆë¬¸ì„ ë²„íŠ¼ìœ¼ë¡œ í‘œì‹œ (í‚¤ ê°’ì— ì¸ë±ìŠ¤ ì¶”ê°€ë¡œ ì¤‘ë³µ ë°©ì§€)
                    if st.button(
                        question, 
                        key=f"question_btn_{idx}",
                        help="í´ë¦­í•˜ë©´ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤",
                        use_container_width=True
                    ):
                        # ë²„íŠ¼ì´ í´ë¦­ë˜ë©´ í•´ë‹¹ ì§ˆë¬¸ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
                        if st.session_state.vectorstore and openai_api_key:
                            # ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ì¶”ê°€
                            st.session_state.messages.append({"role": "user", "content": question})
                            
                            # ë‹µë³€ ìƒì„±
                            with st.spinner("ğŸ¤– ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                # ì„ì‹œ ì»¨í…Œì´ë„ˆ ìƒì„± (ì‹¤ì œë¡œ í‘œì‹œí•˜ì§€ ì•Šê¸° ìœ„í•´)
                                temp_container = st.empty()
                                ai_response = get_rag_response_streaming(
                                    question,
                                    st.session_state.vectorstore,
                                    openai_api_key,
                                    temp_container
                                )
                                temp_container.empty()  # ì„ì‹œ ì»¨í…Œì´ë„ˆ ì •ë¦¬
                            
                            # AI ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
                            st.session_state.messages.append({"role": "assistant", "content": ai_response})
                            
                            # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ì—…ë°ì´íŠ¸
                            st.rerun()
            
            # ë” ë§ì€ ì§ˆë¬¸ ìƒì„± ë²„íŠ¼ ì¶”ê°€
            st.markdown("---")
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button(
                    "ğŸ”„ ë” ë§ì€ ì§ˆë¬¸ ìƒì„±í•˜ê¸°",
                    key="generate_more_questions",
                    help="ì¶”ê°€ì ì¸ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤",
                    use_container_width=True,
                    type="secondary"
                ):
                    if st.session_state.vectorstore and openai_api_key:
                        with st.spinner("ğŸ¤” ì¶”ê°€ í•µì‹¬ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            # ê¸°ì¡´ ì§ˆë¬¸ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ ìƒì„±
                            new_questions = generate_additional_questions(
                                st.session_state.vectorstore, 
                                openai_api_key, 
                                st.session_state.recommended_questions
                            )
                            
                            if new_questions:
                                # ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ì„ ê¸°ì¡´ ì§ˆë¬¸ë“¤ì— ì¶”ê°€
                                st.session_state.recommended_questions.extend(new_questions)
                                st.success(f"âœ… {len(new_questions)}ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ë“¤ í‘œì‹œ
                            else:
                                st.warning("âš ï¸ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    else:
                        st.error("âŒ ë¬¸ì„œê°€ ë¶„ì„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        st.markdown("### ğŸ’¬ ì§ì ‘ ì§ˆë¬¸í•˜ê¸°")
        st.info("ğŸ“ ìœ„ ì§ˆë¬¸ë“¤ ì™¸ì—ë„ ë¬¸ì„œì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
    
    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_question := st.chat_input(placeholder="ì—…ë¡œë“œëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”... ğŸ’¬"):
        # API í‚¤ í™•ì¸
        if not openai_api_key:
            st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”! .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        elif st.session_state.vectorstore is None:
            st.error("ë¬¸ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(user_question)
            st.session_state.messages.append({"role": "user", "content": user_question})

            # AI ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
            with st.chat_message("assistant"):
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±
                message_container = st.empty()
                
                # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                ai_response = get_rag_response_streaming(
                    user_question, 
                    st.session_state.vectorstore, 
                    openai_api_key,
                    message_container
                )
                
            st.session_state.messages.append({"role": "assistant", "content": ai_response})

# í•˜ë‹¨ì— ì‚¬ìš© íŒ ì¶”ê°€
if uploaded_file:
    st.divider()
    with st.expander("ğŸ’¡ ì‚¬ìš© íŒ"):
        st.markdown(f"""
        **í˜„ì¬ ë¶„ì„ ì¤‘ì¸ íŒŒì¼: {uploaded_file.name}**
        **ì‹œìŠ¤í…œ í™˜ê²½: {SYSTEM_OS.title()}** {'(ë°°í¬í™˜ê²½)' if IS_DEPLOYMENT else '(ë¡œì»¬í™˜ê²½)'}
        
        **ì´ ì±—ë´‡ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?**
        - ğŸ“„ ì—…ë¡œë“œëœ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
        - ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
        - âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤
        
        **ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ íŒ:**
        - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
        - ë¬¸ì„œì˜ íŠ¹ì • ë¶€ë¶„ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
        - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """)

    # ë””ë²„ê¹…ìš© (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
    if st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ"):
        st.json({
            "ë©”ì‹œì§€ ê°œìˆ˜": len(st.session_state.messages),
            "OS": SYSTEM_OS,
            "ë°°í¬í™˜ê²½": IS_DEPLOYMENT,
            "Python ë²„ì „": sys.version
        })
        with st.expander("ì „ì²´ ëŒ€í™” ë‚´ì—­"):
            st.json(st.session_state.messages)